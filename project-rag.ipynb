{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from crewai_tools import PDFSearchTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from crewai_tools import tools\n",
    "from crewai import tools\n",
    "from crewai import Crew \n",
    "from crewai import Task \n",
    "from crewai import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY \"] = \"GROQ_API_KEY \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"https://api.groq.com/openai/v1\",\n",
    "    openai_api_key=os.environ[\"GROQ_API_KEY\"],\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LETS DOWNLOAD THE PAPER FROM THE INTERNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762\"\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open (\"attention_is_all_you_need_paper.pdf\", \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Retrieval Object (Knowledge Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_tool = PDFSearchTool(\n",
    "    pdf=\"attention_is_all_you_need_paper.pdf\",\n",
    "    config=dict(\n",
    "        llm=dict(\n",
    "            provider=\"groq\",  # or google, openai, anthropic, llama2, ...\n",
    "            config=dict(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                # temperature=0.5,\n",
    "                # top_p=1,\n",
    "                # stream=True,\n",
    "            ),\n",
    "        ),\n",
    "        embedder=dict(\n",
    "            provider=\"huggingface\",  # <-- Fixed: added '='\n",
    "            config=dict(\n",
    "                model=\"BAAI/bge-small-en-v1.5\",\n",
    "                # task_type=\"retrieval_document\",\n",
    "                # title=\"Embeddings\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Search a PDF's content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Relevant Content:\\nFigure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [ 11] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x), where Sublayer( x)is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . Decoder: The decoder is also composed of a stack of N= 6identical layers.\\n\\nvariety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35]. Here, the encoder maps an input sequence of symbol representations (x1, ., x n)to a sequence\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N d model dff h d k dvPdrop Ïµlstrain PPL BLEU params steps (dev) (dev) Ã—106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A)1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B)16 5.16 25.1 58 32 5.01 25.4 60 (C)2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D)0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_tool.run(\"What is transformer mechanism in 10 words?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Integrate our Search Tool (TAVILY API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)',\n",
       "  'content': 'A transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\".[1] Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel [...] Exact dimension counts within an attention head module\\nThe attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \\nð‘Š\\nð‘„\\n, the key weights \\nð‘Š\\nð¾\\n, and the value weights \\nð‘Š\\nð‘‰\\n.\\nThe module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \\nâ„“\\nseq, query\\n, and each entry is a vector of dimension \\nð‘‘ [...] multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.'},\n",
       " {'url': 'https://medium.com/@mohamedhedifkir/the-transformer-bdfa3366b547',\n",
       "  'content': 'â€œ Duct tape works anywhere. Duct tape is magic and should be worshiped.â€\\n\\nOur vocabulary is therefore made up of 10 words: â€œductâ€, â€œtapeâ€, â€œworksâ€, â€œanywhereâ€, â€œisâ€, â€œmagicâ€, â€œandâ€, â€œshouldâ€, â€œbeâ€, â€œworshiped.â€ We can use a vector of 10 bits to represent any of these words, [...] Sign up\\n\\nSign in\\n\\nSign up\\n\\nSign in\\n\\nThe Transformer\\n\\nM. Hedi Elfkir\\n\\nFollow\\n\\n--\\n\\nListen\\n\\nShare\\n\\nThe Transformer NLP model introduced an â€˜attentionâ€™ mechanism that takes into account the relationship between all the words in the sentence. It creates differential weightings indicating which other elements in the sentence are most critical to the interpretation of a problem word. In this way ambiguous elements can be resolved quickly and efficiently. [...] In language models , This type of representation is used and allow the model to bring similar words closer together and different words away .\\n\\nSelf attention over input embeddings\\n\\nOne problem of the distributed representation of words is that one word will always will have the same representation without really considering the context , here comes the attention mechanism in place . using This sort of mechanism we will analyze and adjust the representation given the global context of the word'},\n",
       " {'url': 'https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power',\n",
       "  'content': \"Additionally, the Transformer's encoder-decoder architecture balances the effect and efficiency of the model.\\n\\nA Transformer is a deep learning model that adopts the self-attention mechanism. This model also analyzes the input data by weighting each component differently.\\n\\nIt is used primarily in artificial intelligence (AI) and natural language processing (NLP) with computer vision (CV).\"},\n",
       " {'url': 'https://www.grammarly.com/blog/ai/what-is-a-transformer-model/',\n",
       "  'content': 'Parallelizability\\n\\nThe attention mechanism in transformers can be executed in parallel across all tokens in the input sequence. This contrasts with RNNs, which process tokens sequentially. As a result, transformers can be trained and deployed more quickly, providing faster responses to users. This parallel processing capability significantly enhances the efficiency of transformers compared to RNNs.\\n\\nScalability [...] Attention is a mechanism in transformers that enables them to process inputs efficiently and maintain information over long sequences (e.g., an entire essay). [...] 1\\nTokenization: Turns the text into tokens (similar to breaking down a sentence into individual words).\\n\\n2\\n Embedding: Converts the tokens into vectors, incorporating positional embeddings so the model understands the tokenâ€™s location in the input.\\n\\n3\\n Attention mechanism: Processes the tokens using self-attention (for input tokens) or cross-attention (between input tokens and generated tokens). This mechanism allows the model to weigh the importance of different tokens when generating output.'},\n",
       " {'url': 'https://poloclub.github.io/transformer-explainer/',\n",
       "  'content': 'Fundamentally, text-generative Transformer models operate on the principle of next-word prediction: given a text prompt from the user, what is the most probable next word that will follow\\n\\t\\t\\tthis input? The core innovation and power of Transformers lie in their use of self-attention mechanism,\\n\\t\\t\\twhich allows them to process entire sequences and capture long-range dependencies more effectively\\n\\t\\t\\tthan previous architectures.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search_tool.run(\"What is transformer mechanism in 10 words?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOOLS TO DETERMINE WHERE TO USE VECTOR STORE OR WEBSEARCGH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def router_tool(question):\n",
    "    \"\"\"Router Function\"\"\"\n",
    "    if 'self-attention' in question:\n",
    "        return 'vectorstore'\n",
    "    else:\n",
    "        return 'web_search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUTER AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Router_Agent = Agent(\n",
    "    role = \"Router\",\n",
    "    goal = 'Route user question to a vector or web search',\n",
    "    backstory = (\n",
    "        \"You are a expert at routing a user question to a vector store or web search.\"\n",
    "        \"Use the vectorestore for questions on concept related to Retrieval-Augmented Generation.\"\n",
    "        \"You do not need to be stringent with the keywords in the question related to these topic. Otherwise, use web-search.\"   \n",
    "    ),\n",
    "    verbose = True,\n",
    "    allow_delegation= False,\n",
    "    llm = llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVAL AGENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Retriever_Agent = Agent(\n",
    "    role = \"Retriever\",\n",
    "    goal = 'Use the information retrieved from the vectorstore to answer the question',\n",
    "    backstory = (\n",
    "        \"You are an assistant for question-answer task.\"\n",
    "        \"Use the information present in the retrieved context to answer the question.\"\n",
    "        \"You have to provide a clear concise answer.\"   \n",
    "    ),\n",
    "    verbose = True,\n",
    "    allow_delegation= False,\n",
    "    llm = llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADER AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grader_agent = Agent(\n",
    "    role = 'Answer Grader',\n",
    "    goal = 'Filter out erroneous retrieval',\n",
    "    backstory = (\n",
    "        \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
    "        \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
    "        \"It does not need to be a strinent test. You have to make sure that the answer is relevant to the question.\"   \n",
    "    ),\n",
    "    verbose = True,\n",
    "    allow_delegation= False,\n",
    "    llm = llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HALLUCINATION GRADER AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_grader = Agent(\n",
    "    role = 'Hallucination Grader',\n",
    "    goal = 'Filter out hallucination',\n",
    "    backstory = (\n",
    "        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
    "        \"Make sure you meticulously review the answer and check if the response provided is in alignment with the question asked.\" \n",
    "    ),\n",
    "    verbose = True,\n",
    "    allow_delegation= False,\n",
    "    llm = llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER GRADER AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_grader = Agent(\n",
    "    role = 'Answer Grader',\n",
    "    goal = 'Filter out hallucination from the answer',\n",
    "    backstory = (\n",
    "        \"You are a grader assessing whether an answer is useful to resolve a question\"\n",
    "        \"Make sure you meticulously review the answer and check if the response provided is in alignment with the question asked.\" \n",
    "        \"If the answer is relevant generate a clear and concise response.\"\n",
    "        \"If the answer generated is not relevant then perform a websearch using 'web_search_tool' \"\n",
    "    ),\n",
    "    verbose = True,\n",
    "    allow_delegation= False,\n",
    "    llm = llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUTER TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_task = Task(\n",
    "    description=(\n",
    "        \"Analyze the keywords in the question: {question}. \"\n",
    "        \"Based on the keywords, decide whether it is eligible for a vectorstore search or a web search. \"\n",
    "        \"Return a single word 'vectorstore' if it is eligible for vectorstore search. \"\n",
    "        \"Return a single word 'websearch' if it is eligible for web search. \"\n",
    "        \"Do not provide any other preamble or explanation.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Give a binary choice: 'websearch' or 'vectorstore'. \"\n",
    "        \"Do not provide any other preamble or explanation.\"\n",
    "    ),\n",
    "    agent=Router_Agent,\n",
    "    tools=[router_tool],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVER TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_task = Task(\n",
    "    description=(\n",
    "        \"Your job is to retrieve relevant information for the question: {question}. \"\n",
    "        \"If the previous output (from the routing step) was 'websearch', use the web_search_tool. \"\n",
    "        \"If it was 'vectorstore', use the rag_tool. \"\n",
    "        \"Do not call any task. Only use the tools provided to gather information.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Return a clear and concise answer to the question. \"\n",
    "        \"Choose the appropriate tool (web_search_tool or rag_tool) based on the previous routing decision.\"\n",
    "    ),\n",
    "    agent=Retriever_Agent,\n",
    "    context=[router_task],\n",
    "    tools=[web_search_tool, rag_tool],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADER TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_task = Task(\n",
    "    description=(\n",
    "        \"Based on the response from the retriever task and the original question: {question}, \"\n",
    "        \"evaluate whether the retrieved content is aligned with the question.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Return a binary score: 'yes' or 'no'. \"\n",
    "        \"Return 'yes' if the response from 'retriever_task' aligns with the question. \"\n",
    "        \"Return 'no' if it does not. \"\n",
    "        \"Do not provide any other explanation or preamble.\"\n",
    "    ),\n",
    "    agent=Grader_agent,\n",
    "    context=[retriever_task],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HALLUCINATION TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_task = Task(\n",
    "    description=(\n",
    "        \"Based on the response from the retriever task and the original question: {question}, \"\n",
    "        \"evaluate whether the answer is grounded in fact or hallucinated.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Return a binary score: 'yes' or 'no'. \"\n",
    "        \"Return 'yes' if the answer is useful and factually aligned with the question. \"\n",
    "        \"Return 'no' if the answer is not useful or not factual. \"\n",
    "        \"Do not provide any other explanation or preamble.\"\n",
    "    ),\n",
    "    agent=hallucination_grader,\n",
    "    context=[grader_task],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANSWER TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_task = Task(\n",
    "    description=(\n",
    "        \"Based on the response from the retriever task and the original question: {question}, \"\n",
    "        \"determine whether the answer is useful. \"\n",
    "        \"If the hallucination_task output is 'yes', return a clear and concise final answer. \"\n",
    "        \"If the hallucination_task output is 'no', perform a web search using the web_search_tool and return the response.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"Return a clear and concise answer if the hallucination_task result is 'yes'. \"\n",
    "        \"If it is 'no', perform a web search using the web_search_tool and return the result. \"\n",
    "        \"If no relevant result is found, return: 'Sorry! Unable to find a valid response.'\"\n",
    "    ),\n",
    "    agent=answer_grader,\n",
    "    context=[hallucination_task],\n",
    "    tools=[web_search_tool],  # required if fallback is triggered\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG CREW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:54:07,580 - 8711620672 - __init__.py-__init__:535 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "rag_crew = Crew(\n",
    "    agents = [Router_Agent, Router_Agent, Grader_agent, hallucination_grader, answer_grader],\n",
    "    tasks = [router_task, retriever_task, grader_task, hallucination_task, answer_task],\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyze the keywords in the question: How does self-attention mechanism help large language models?. Based on the keywords, decide whether it is eligible for a vectorstore search or a web search. Return a single word 'vectorstore' if it is eligible for vectorstore search. Return a single word 'websearch' if it is eligible for web search. Do not provide any other preamble or explanation.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The question seems to be related to the concept of self-attention mechanism in large language models, which is a topic in the field of Retrieval-Augmented Generation.\n",
      "Action: router_tool\n",
      "Action Input: {\"question\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[95m \n",
      "\n",
      "vectorstore\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: vectorstore\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: vectorstore\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Your job is to retrieve relevant information for the question: How does self-attention mechanism help large language models?. If the previous output (from the routing step) was 'websearch', use the web_search_tool. If it was 'vectorstore', use the rag_tool. Do not call any task. Only use the tools provided to gather information.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to retrieve relevant information about the self-attention mechanism and its role in large language models.\n",
      "\n",
      "Action: Search a PDF's content\n",
      "\n",
      "Action Input: {\"query\": \"self-attention mechanism\", \"file\": \"attention_is_all_you_need_paper.pdf\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "Relevant Content:\n",
      "the foundation of the Extended Neural GPU [16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a\n",
      "\n",
      "values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. â€¢The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. â€¢Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to âˆ’âˆž) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3\n",
      "\n",
      "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum 3\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "Self-attention mechanism helps large language models by allowing every position in the decoder to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms in sequence-to-sequence models. This enables the model to capture long-range dependencies and relationships between different parts of the input sequence. Additionally, self-attention layers in the decoder allow each position to attend to all positions in the decoder up to and including that position, while preventing leftward information flow to preserve the auto-regressive property.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: Self-attention mechanism helps large language models by allowing every position in the decoder to attend over all positions in the input sequence, mimicking the typical encoder-decoder attention mechanisms in sequence-to-sequence models. This enables the model to capture long-range dependencies and relationships between different parts of the input sequence. Additionally, self-attention layers in the decoder allow each position to attend to all positions in the decoder up to and including that position, while preventing leftward information flow to preserve the auto-regressive property.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task and the original question: How does self-attention mechanism help large language models?, evaluate whether the retrieved content is aligned with the question.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
      "Final Answer: yes\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: yes\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Hallucination Grader\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task and the original question: How does self-attention mechanism help large language models?, evaluate whether the answer is grounded in fact or hallucinated.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I will meticulously review the answer and check if the response provided is in alignment with the question asked. I will filter out any hallucinations and provide a binary score of 'yes' if the answer is grounded in fact and 'no' if it is not.\n",
      "\n",
      "Final Answer: no\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Hallucination Grader] Task output: no\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task and the original question: How does self-attention mechanism help large language models?, determine whether the answer is useful. If the hallucination_task output is 'yes', return a clear and concise final answer. If the hallucination_task output is 'no', perform a web search using the web_search_tool and return the response.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer to determine if it's useful in resolving the question \"How does self-attention mechanism help large language models?\"\n",
      "\n",
      "Action: web_search_tool\n",
      "Action Input: {\"query\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'web_search_tool' don't exist, these are the only available Actions: tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer to determine if it's useful in resolving the question \"How does self-attention mechanism help large language models?\"\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[95m \n",
      "\n",
      "[{'url': 'https://medium.com/data-science-community-srm/exploring-attention-mechanisms-in-large-language-models-33549ae2c5b8', 'content': \"The self-attention mechanism lets the model look at each friend's message, and based on what they said, decide how much attention to give to\"}, {'url': 'https://magnimindacademy.com/blog/the-mechanism-of-attention-in-large-language-models-a-comprehensive-guide/', 'content': 'So this attention mechanism is undoubtedly the backbone of large language models as we know them and has reshaped\\u2002the field of natural language processing (NLP) â€” changing the way we think about how machines process and generate text. Attention mechanisms have allowed AI to better understand\\u2002and emulate human language, by enabling models to selectively focus on the parts of an input sequence that are most salient. It is the introduction of self-attention, another fundamental component, and [...] Example:\\nConsider the sentence:\\nThe cat\\u2002sat on the mat, which was black.â€\\nSelf-attention allows\\u2002the model to discern that the word â€œblackâ€ refers to the word â€œcatâ€, not the word â€œmatâ€. The Query vector of â€œblackâ€ targets\\u2002its relevant pair, Key vector of â€œcat,â€ with a high attention weight between them [15]. [...] At their core, Transformer models\\u2002make use of a self-attention (or intra-attention) mechanism. It allows every word in a\\u2002sentence to reach out, in a sort of interactive way, and decide how much weight one word should give another. This is needed to build up context and relations over tokens in the natural way as it would for the previous\\u2002set of tokens (even across long sequences. [12]\\nHow Self-Attention Works\\nSeveral stages characterize the\\u2002process of self-attention:'}, {'url': 'https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention', 'content': \"13\\nShare\\nThis article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models. [...] We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input's context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.\"}, {'url': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html', 'content': 'We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the inputâ€™s context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document. [...] Since its introduction via the original transformer paper (Attention Is All You Need), self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing (NLP). Since self-attention is now everywhere, itâ€™s important to understand how it works. [...] In this article, we saw how self-attention works using a step-by-step coding approach. We then extended this concept to multi-head attention, the widely used component of large-language transformers. After discussing self-attention and multi-head attention, we introduced yet another concept: cross-attention, which is a flavor of self-attention that we can apply between two different sequences. This is already a lot of information to take in. Letâ€™s leave the training of a neural network using'}, {'url': 'https://www.linkedin.com/pulse/what-self-attention-impact-large-language-models-llm-nikhil-goel-srpbc', 'content': 'This learning of context or relationship or one word with another is enabled by Self-Attention mechanism. Through Self Attention Neural Nets understand the'}]\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer to determine if it's useful in resolving the question \"How does self-attention mechanism help large language models?\"\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer to determine if it's useful in resolving the question \"How does self-attention mechanism help large language models?\"\n",
      "\n",
      "Action: web_search_tool\n",
      "Action Input: {\"query\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'web_search_tool' don't exist, these are the only available Actions: tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer to determine if it's useful in resolving the question \"How does self-attention mechanism help large language models?\"\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer to determine if it's useful in resolving the question \"How does self-attention mechanism help large language models?\"\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"How does self-attention mechanism help large language models?\"}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to think about what to do next.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "\n",
      "Action Input: {\"query\": \"How does self-attention mechanism help large language models?\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to think about what to do next.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "\n",
      "Action Input: {\"query\": \"What is the self-attention mechanism and how does it help large language models?\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "[{'url': 'https://medium.com/data-science-community-srm/exploring-attention-mechanisms-in-large-language-models-33549ae2c5b8', 'content': \"The self-attention mechanism lets the model look at each friend's message, and based on what they said, decide how much attention to give to\"}, {'url': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html', 'content': 'We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the inputâ€™s context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document. [...] Since its introduction via the original transformer paper (Attention Is All You Need), self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing (NLP). Since self-attention is now everywhere, itâ€™s important to understand how it works. [...] In this article, we saw how self-attention works using a step-by-step coding approach. We then extended this concept to multi-head attention, the widely used component of large-language transformers. After discussing self-attention and multi-head attention, we introduced yet another concept: cross-attention, which is a flavor of self-attention that we can apply between two different sequences. This is already a lot of information to take in. Letâ€™s leave the training of a neural network using'}, {'url': 'https://magnimindacademy.com/blog/the-mechanism-of-attention-in-large-language-models-a-comprehensive-guide/', 'content': 'So this attention mechanism is undoubtedly the backbone of large language models as we know them and has reshaped\\u2002the field of natural language processing (NLP) â€” changing the way we think about how machines process and generate text. Attention mechanisms have allowed AI to better understand\\u2002and emulate human language, by enabling models to selectively focus on the parts of an input sequence that are most salient. It is the introduction of self-attention, another fundamental component, and [...] At their core, Transformer models\\u2002make use of a self-attention (or intra-attention) mechanism. It allows every word in a\\u2002sentence to reach out, in a sort of interactive way, and decide how much weight one word should give another. This is needed to build up context and relations over tokens in the natural way as it would for the previous\\u2002set of tokens (even across long sequences. [12]\\nHow Self-Attention Works\\nSeveral stages characterize the\\u2002process of self-attention: [...] Example:\\nConsider the sentence:\\nThe cat\\u2002sat on the mat, which was black.â€\\nSelf-attention allows\\u2002the model to discern that the word â€œblackâ€ refers to the word â€œcatâ€, not the word â€œmatâ€. The Query vector of â€œblackâ€ targets\\u2002its relevant pair, Key vector of â€œcat,â€ with a high attention weight between them [15].'}, {'url': 'https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention', 'content': \"13\\nShare\\nThis article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models. [...] We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input's context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.\"}, {'url': 'https://www.linkedin.com/pulse/what-self-attention-impact-large-language-models-llm-nikhil-goel-srpbc', 'content': 'This learning of context or relationship or one word with another is enabled by Self-Attention mechanism. Through Self Attention Neural Nets understand the'}]\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer to determine if it's useful in resolving the question \"How does self-attention mechanism help large language models?\"\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "\n",
      "Action Input: {\"query\": \"What is the self-attention mechanism and how does it help large language models?\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "\n",
      "Final Answer: The self-attention mechanism helps large language models by enabling them to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document. The self-attention mechanism allows the model to selectively focus on the parts of an input sequence that are most salient, and to build up context and relations over tokens in the natural way as it would for the previous set of tokens (even across long sequences).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: The self-attention mechanism helps large language models by enabling them to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document. The self-attention mechanism allows the model to selectively focus on the parts of an input sequence that are most salient, and to build up context and relations over tokens in the natural way as it would for the previous set of tokens (even across long sequences).\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"How does self-attention mechanism help large language models?\"}\n",
    "result = rag_crew.kickoff(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The self-attention mechanism helps large language models by enabling them to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document. The self-attention mechanism allows the model to selectively focus on the parts of an input sequence that are most salient, and to build up context and relations over tokens in the natural way as it would for the previous set of tokens (even across long sequences).\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Router\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Analyze the keywords in the question: Explain LLM using the web_search?. Based on the keywords, decide whether it is eligible for a vectorstore search or a web search. Return a single word 'vectorstore' if it is eligible for vectorstore search. Return a single word 'websearch' if it is eligible for web search. Do not provide any other preamble or explanation.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to analyze the keywords in the question to determine whether it's eligible for a vectorstore search or a web search.\n",
      "\n",
      "Action: router_tool\n",
      "\n",
      "Action Input: {\"question\": \"Explain LLM using the web_search?\"}\n",
      "\u001b[0m\u001b[95m \n",
      "\n",
      "web_search\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mFinal Answer: websearch\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Router] Task output: websearch\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Retriever\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Your job is to retrieve relevant information for the question: Explain LLM using the web_search?. If the previous output (from the routing step) was 'websearch', use the web_search_tool. If it was 'vectorstore', use the rag_tool. Do not call any task. Only use the tools provided to gather information.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use the web_search_tool to gather information about LLM.\n",
      "\n",
      "Action: web_search_tool\n",
      "Action Input: {\"query\": \"Explain LLM\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'web_search_tool' don't exist, these are the only available Actions: tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "Search a PDF's content: A tool that can be used to semantic search a query the attention_is_all_you_need_paper.pdf PDF's content.\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a tool to gather information about LLM.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"Explain LLM\"}\u001b[0m\u001b[95m \n",
      "\n",
      "[{'url': 'https://www.balbix.com/insights/what-is-large-language-model-llm/', 'content': 'A Large Language Model (LLM) is an advanced form of artificial intelligence designed to understand, generate, and manipulate human language on a large scale. LLMs are trained on vast amounts of textual data, enabling them to produce coherent and contextually relevant responses to various prompts.\\n\\nIn summary, LLMs are AI systems that can read, comprehend, and generate text like humans, making interactions with technology more natural and intuitive. [...] This article will explain how they work, how theyâ€™re trained, what theyâ€™re used for, their upsides and downsides, and how theyâ€™re used in cybersecurity.\\n\\nHow Do LLMs Work?\\n\\nLLMs operate through sophisticated machine learning techniques, using massive computational resources. [...] Large Language Models (LLMs) are transforming society, promising to enhance our lives in both personal and professional aspects. By enabling machines to understand and generate human-like language, LLMs are bridging the gap between humans and computers, unlocking new possibilities across various industries.\\n\\nEnhanced Natural Language Understanding'}, {'url': 'https://en.wikipedia.org/wiki/Large_language_model', 'content': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\\nUnlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time. [...] Large language models are incredibly flexible. One model can perform completely different tasks such as answering questions, summarizing documents, translating languages and completing sentences. LLMs have the potential to disrupt content creation and the way people use search engines and virtual assistants.'}, {'url': 'https://www.sap.com/resources/what-is-large-language-model', 'content': 'A large language model (LLM) is a type of artificial intelligence (AI) that excels at processing, understanding, and generating human language.'}, {'url': 'https://www.youtube.com/watch?v=5sLYAQS9sWQ', 'content': \"Large language models-- or LLMs --are a type of generative pretrained transformer (GPT) that can create human-like text and code. There's a lot of talk about GPTs and LLMs lately, but they've actually been around for years! In this video, Martin Keen briefly explains what a LLM is, how they relate to foundation models, and then covers how they work and how they can be used to address various business problems.\"}]\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a tool to gather information about LLM.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"Explain LLM\"}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a tool to gather more information about LLM.\n",
      "\n",
      "Action: Search a PDF's content\n",
      "Action Input: {\"query\": \"the attention_is_all_you_need_paper.pdf\", \"search\": \"Large Language Model\"}\u001b[0m\u001b[95m \n",
      "\n",
      "Relevant Content:\n",
      "Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word â€˜itsâ€™ for attention heads 5 and 6. Note that the attentions are very sharp for this word. 14\n",
      "\n",
      "Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult . <EOS> <pad> <pad> <pad> <pad> <pad> <pad> Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb â€˜makingâ€™, completing the phrase â€˜making.more difficultâ€™. Attentions here shown only for the word â€˜makingâ€™. Different colors represent different heads. Best viewed in color. 13\n",
      "\n",
      "in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "A Large Language Model (LLM) is an advanced form of artificial intelligence designed to understand, generate, and manipulate human language on a large scale. LLMs are trained on vast amounts of textual data, enabling them to produce coherent and contextually relevant responses to various prompts. In summary, LLMs are AI systems that can read, comprehend, and generate text like humans, making interactions with technology more natural and intuitive.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Retriever] Task output: A Large Language Model (LLM) is an advanced form of artificial intelligence designed to understand, generate, and manipulate human language on a large scale. LLMs are trained on vast amounts of textual data, enabling them to produce coherent and contextually relevant responses to various prompts. In summary, LLMs are AI systems that can read, comprehend, and generate text like humans, making interactions with technology more natural and intuitive.\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task and the original question: Explain LLM using the web_search?, evaluate whether the retrieved content is aligned with the question.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now can give a great answer\n",
      "\n",
      "Final Answer: yes\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: yes\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Hallucination Grader\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task and the original question: Explain LLM using the web_search?, evaluate whether the answer is grounded in fact or hallucinated.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I will carefully review the response from the retriever task and the original question to assess whether the answer is grounded in fact or hallucinated. I will use my expertise as a hallucination grader to evaluate the answer and provide a binary score of \"yes\" or \"no\" based on the criteria provided.\n",
      "\n",
      "Final Answer: no\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Hallucination Grader] Task output: no\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[1m\u001b[95m [DEBUG]: == Working Agent: Answer Grader\u001b[00m\n",
      "\u001b[1m\u001b[95m [INFO]: == Starting Task: Based on the response from the retriever task and the original question: Explain LLM using the web_search?, determine whether the answer is useful. If the hallucination_task output is 'yes', return a clear and concise final answer. If the hallucination_task output is 'no', perform a web search using the web_search_tool and return the response.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new CrewAgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer and determine if it's useful in resolving the question.\n",
      "\n",
      "Action: hallucination_task\n",
      "Action Input: {\"question\": \"Explain LLM using the web_search?\"}\u001b[0m\u001b[91m \n",
      "\n",
      "Action 'hallucination_task' don't exist, these are the only available Actions: tavily_search_results_json: A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer and determine if it's useful in resolving the question.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"Explain LLM using the web_search\"}\u001b[0m\u001b[95m \n",
      "\n",
      "[{'url': 'https://discuss.huggingface.co/t/how-to-use-website-search-functionality-with-my-llm/135257', 'content': \"Hello. There are two broad methods. One is to process the results of a normal web search using a programming language such as Python and pass the results to LLM yourself. The other is a method called Function Calling, in which you instruct LLM to execute a search tool and return the results. (There are various names for this method.) [...] In the case of the former, there are various useful libraries, so you should try searching for them. If you can use the latter, it is usually built into the LLM execution environment, so it is often found somewhere in the documentation. [...] Topic |  | Replies | Views | Activity\\nWeb Search Implementation with LLMResearch | 1 | 1056 | October 11, 2024\\nDarshan Hiranandani : Implementing Website Search Functionality for Custom Chatbots Using RAGBeginners | 2 | 79 | January 14, 2025\\nFunction Call via HuggingFaceLLMIntermediate | 1 | 245 | August 22, 2024\\nFunction/tool calling using Transformer modelsðŸ¤—Transformers | 3 | 221 | February 24, 2025\\nHow to use llm model's api?Beginners | 2 | 1685 | November 14, 2024\"}, {'url': 'https://www.ml6.eu/blogpost/how-llms-access-real-time-data-from-the-web', 'content': 'Additionally, it should be clear that if youÃ¢\\x80\\x99re building a separate software solution that talks directly to the red block, the LLM API (e.g. the GPT-4 API), you will not be getting the sweet benefits of the Web Knowledge Access functionality. We will look into why that is below but first, we have to build a mutual understanding of the Web Search Process.\\n\\nÃ¢\\x80\\x8d\\n\\nHow players approach the Web Search game\\n\\nÃ¢\\x80\\x8d [...] But as discussed, these LLMs cannot browse the internet themselves, so when you pose a question on how Ridley Scott has performed in the very recent Oscars, Billy the LLM wonÃ¢\\x80\\x99t know that from the knowledge it was trained on. However, luckily for us, the controlling layer of the Web UI is able to do a quick search of the web (just like a human would), fetch the right information and deliver that to the LLM as context to form its response with. [...] ThereÃ¢\\x80\\x99s one crucial note to make here when thinking about leveraging this search functionality: as a user, you have NO control on how the search flow works. You can be impacted:'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] Large language models are incredibly flexible. One model can perform completely different tasks such as answering questions, summarizing documents, translating languages and completing sentences. LLMs have the potential to disrupt content creation and the way people use search engines and virtual assistants. [...] Text classification\\nUsing clustering, LLMs can classify text with similar meanings or sentiments. Uses include measuring customer sentiment, determining the relationship between texts, and document search.\\nCode generation'}, {'url': 'https://medium.com/@avikumart_/automating-web-search-using-langchain-and-google-search-apis-8cbed2a1c8bf', 'content': 'First, we will look at the typical web research and automation application workflow which is crucial to understand architecture of such LLM powered applications. When user queries to the web research automation application, Google Search API takes query and returns number of web links that will be loaded using web loader which scraps the web pages. loaded web content will then be transformed into readable text format by removing all unwanted html tags. Letâ€™s look the below diagram for more [...] For more details on how to set above mentioned API keys. Visit this link for further guidance. Now in next steps we will initialise the vector store, LLM and Google search instances. following this step we will initialise the web retrieval tool to use LLM for generation of multiple queries which will execute search on google search APIs to bring web links relevant to the queries. then, these web pages will be scraped and loaded into vector stores which is going to be used as context for answer [...] Now, Developing Q&A application for automating web research using queries can be archived via Google Search APIs and methods like â€˜WebResearchRetrieverâ€™ from LangChain framework. To begin with the application development first we will look the the application workflow diagram to understand key components of the such LLM based application.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'An LLM, or large language model, is a machine learning model that can comprehend and generate human language. Learn how LLM models work.'}]\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer and determine if it's useful in resolving the question.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"Explain LLM using the web_search\"}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to assess the answer and determine if it's useful in resolving the question.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"Explain LLM using the web_search\"}\u001b[0m\u001b[95m \n",
      "\n",
      "I tried reusing the same input, I must stop using this action input. I'll try something else instead.\n",
      "\n",
      "\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to think about what to do next.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: {\"query\": \"What is LLM?\"}\u001b[0m\u001b[95m \n",
      "\n",
      "[{'url': 'https://en.wikipedia.org/wiki/Large_language_model', 'content': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.'}, {'url': 'https://www.sap.com/resources/what-is-large-language-model', 'content': 'A large language model (LLM) is a type of artificial intelligence (AI) that excels at processing, understanding, and generating human language.'}, {'url': 'https://aws.amazon.com/what-is/large-language-model/', 'content': 'Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it. [...] LLMs are a disruptive factor that will change the workplace. LLMs will likely reduce monotonous and repetitive tasks in the same way that robots did for repetitive manufacturing tasks. Possibilities include repetitive clerical tasks, customer service chatbots, and simple automated copywriting.\\nConversational AI [...] While not perfect, LLMs are demonstrating a remarkable ability to make predictions based on a relatively small number of prompts or inputs. LLMs can be used for generative AI (artificial intelligence) to produce content based on input prompts in human language.\\nLLMs are big, very big. They can consider billions of parameters and have many possible uses. Here are some examples:'}, {'url': 'https://www.elastic.co/what-is/large-language-models', 'content': 'A large language model (LLM) is a deep learning algorithm that can perform a variety of natural language processing (NLP) tasks.'}, {'url': 'https://www.cloudflare.com/learning/ai/what-is-large-language-model/', 'content': 'An LLM, or large language model, is a machine learning model that can comprehend and generate human language. Learn how LLM models work.'}]\n",
      "\u001b[00m\n",
      "\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "\n",
      "Final Answer: \n",
      "\n",
      "Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[1m\u001b[92m [DEBUG]: == [Answer Grader] Task output: Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
      "\n",
      "\u001b[00m\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Explain LLM using the web_search?\"}\n",
    "result = rag_crew.kickoff(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models, also known as LLMs, are very large deep learning models that are pre-trained on vast amounts of data. The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
